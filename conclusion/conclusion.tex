\chapter{Conclusion}\label{Conclusion}

\section{Summary of Results}

This thesis is part of a growing effort applying the use of sentiment analysis techniques to multilingual studies which may discover over time valuable information on how different populations act and think about various topics.

The first step of this thesis involved forming a data set. A corpus including 305,454 news articles and over 167 million words was collected through the use of a scraper which identified French news articles on the topic of COVID-19. The criteria for selection ensured relevance using the mentions of "covid" or "corona" as well as the date of publishing which was set to the range of 1/01/2019 - 25/03/2021. The diverse set of sources and the size of the corpus made it an interesting candidate for this study.

After some background research was performed on the field of natural language processing and sentiment analysis, a lexicon-based system was put in place as it provided consistent results without the need to train a model. Moreover, it enabled the extraction of sentiment proxies using general dictionaries such as \emph{FEEL}, \emph{Diko} or \emph{Polarimots} as well as specialised dictionaries which were built as part of this thesis. The custom dictionaries were built with the goal of extracting topic-specific information as in the context of a pandemic, words such as "virus", or "contamination" should not be treated negatively as they typically are in general lexicons. These custom lexicons were built with the aid of AntConc, a tool used to analyse how terms within a corpus are distributed.

A sentiment analysis pipeline was then put in place to extract sentiment for all of the lexicons on the corpus, articles were loaded in batches of 2,000, then each article was put through a set of operations such as to prepare the text to be matched with the lexicons and finally a set of sentiments were written back to the database such as to enable further analysis with a simple SQL query. 

A script was written which aggregates sentiment data from the database as well as official metrics from the French government into a single CSV file. This step enabled me to perform a statistical analysis on the obtained data using tools such as Excel and GRETL.

In depth analysis of the obtained data show some interesting results. To start, each of the specialised dictionaries were found to included a small subset of words which made up most of the mentions for each dictionary. Where "\emph{vaccin}" and "\emph{vaccination}" makes up for almost 50\% of virus-related words. Two VAR models built as part of this thesis have obtained adjusted $R^2$ values all over 0.8 and the second on over 0.5 which is very high. It is worth noting however than some models have observed values of Durbin-Watson which indicates small amounts of positive autocorrelation. Moreover, the P-value for the F statistics observed are all very small (some in the range of $10^{-161}$), finding bidirectional causality on Granger-Causality tests for all variables tested. With more time on this project, one of the main goals should be to dig further into these results.

\section{Future Work}

As previously mentioned, a large amount of time was spent on data aggregation, this resulted in a large corpus which was ideal for this study. However as a result less time was allocated to perfecting the VAR analysis. The next steps on this project would be to explore more possibilities and optimise the obtained models further. Moreover, those results could potentially lead to some fine tuning of the sentiment analysis pipeline as well as more specialised dictionaries.

The corpus was built from French newspapers articles, resulting in a set of articles which are expected to be written by professional journalists. The language found within the corpus can thus be expected to be grammatically correct and include a general vocabulary. However, given that the corpus focuses on COVID-19 articles, this can be considered a specialist domain thus the effectiveness of generalist dictionaries is not guaranteed. The use of specialist dictionaries removes some of this potential bias but the fact remains that it is a possibility that most of the terms within dictionaries have incorrect polarity. As a result, a virus specialist dictionary may be required in order to validate the results of this thesis.

Finally, the sentiment analysis pipeline which was implemented for this system makes use of a lexicon-based approach. The system could be improved by implementing algorithms such as negation handling as well as a more refined approach to handling context within sentences. Another approach potentially worth exploring may be to use machine learning models as they have been found to yields better results in some situations. The first step to do so would be to start by manually labelling a subset of the corpus and train a variety of popular models including Naive Bayes, Support-Vector-Machines, Neural Nets and more.

\section{Final Comments}

The work carried out throughout this thesis faced a number of challenges, in particular due to the fact that many of the concepts that are core to this study were new to me.

Collecting the articles in order to form a large corpus was met with a number of challenges including rate limiting, a need for batch downloading such as to not make LexisNexis crash or even the number of popups to handle. Incrementally those issues were solved and as a result the scraper system now works entirely autonomously. On top of this, once fixed, it took a week for the scrapper to finish downloading all of the articles which is a considerable amount of time in the scope of this thesis.

Secondly, managing the MySQL corpus came with many learning points with regards to scalability, firstly making sure to set appropriate indexes on table creation was key to ensuring its performance and was only discovered after some very slow queries were being observed. Secondly, batch operations do not need to have the database commit the changes and recomputing core values on every insert or update command. As a result some very valuable efficiency improvements can be made to the database loader system. I thankfully discovered this during the sentiment computing pipeline and as a result writing the computed sentiment for articles back into the database was made faster by a number of magnitudes. Another point which relates to the sentiment analysis pipeline and scalability involves vectorisation. It is the process of transforming words from a string representation into an integer. Implementing vectorisation as part of the sentiment analysis pipeline would have likely also improved the speed at which sentiment was being computed by a number of magnitude. As it was previously mentioned, the time required to perform sentiment analysis on the entire corpus for a single lexicon was of roughly 10 to 16 hours depending on the lexicon size.

Finally, it was observed that the frequency of articles as well as government data when aggregated daily was not particularly constant, seeing considerable variations on the daily number of article counts or recorded daily cases for example. It is very likely that changing the data aggregation method to be an average over a number of days would enable for more precise modelling and more consistent results. This comes with the challenge of reducing the number of data points though.

Overall, I have been introduced to and learned many news topics, tools and concepts throughout this thesis and I am very happy with how it turned out. I personally believe that this project has been a success and I am very happy with the fact that I have learned very important knowledge particularly in the domains of computer linguistics and statistics.
