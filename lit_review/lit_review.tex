\chapter{Literature Review}\label{Literature Review}

\section{The Media}

\subsection{Role of the Media in a crisis}\label{Role of the Media in a crisis}

TODO

\subsection{French Press}\label{chap:French Press}

TODO: possibly include some history of French press, AFP and more
% Find what the role of the press is, should be specifically in the context of important, hurtful events
%How the people see the covid 19 pandemic

\section{Sentiment Analysis}\label{Sentiment Analysis}

Sentiment Analysis (or opinion mining), is "the task of finding the opinions of authors about specific entities" \citep{feldman2013techniques}. It is a sub-field of Natural Language Processing (NLP) which has been growing in interest on a worldwide scale for over 15 years thanks to its vast range of use cases (Figure \ref{fig:sentiment interest}).

\begin{figure}[h!]
      \centering
      \includegraphics[scale=0.65]{lit_review/sentiment_analysis_interest.png}
      \caption{Sentiment Analysis Interest}
      \label{fig:sentiment interest}
      \emph{Source: Google Trends}
\end{figure}

In fact, sentiment analysis has been used in a wide range of topics, including political analyses of social and news media \citep{ahmad2011new}, stock market behaviors \citep{rao2012analyzing,li2014news}, customer sentiment \citep{cambria2013new,mouthami2013sentiment}, fake news detection \citep{bhutani2019fake} and countless more applications.

Languages are full of ambiguity and in particular the French language as some words can have drastically different meanings even though they are written the same, or adding an accent from a large variety of options can drastically change the meaning of a word. On top of this, a select set of words make up the majority of text, in fact, using a frequency-rank, it was found that the first 100 words make up for about 50\% of all of the words in a text cluster \citep{ahmad2007being}, while this number may vary for the french language, this observation means that the interpretation of these highly frequent words is key to extracting sentiment proxies from text. Short text such as tweets \citep{chandrasekaran2020topics} or customer reviews can include a single sentiment or topic. That is not always the case though, as the length of text grows more conflicting opinions are likely found. A simple example of this can be found on a customer review for a product: "I liked using the product, however I wish it didn't break after a day" where clearly both a positive and negative sentiment can be extracted from this review. This shows that there are multiple ways to perform sentiment analysis on text. Document-level computes the overall sentiment of a piece of text such as a tweet or a review, on the other hand, a sentence-level analysis approach can also be taken where a distinction is made between factual sentences (objective phrases) and subjective sentences which mostly contain opinions (subjective phrases) \citep{wilson2009recognizing}. A sentence-level approach is often used for tasks such as text summarizing, on the other hand, a document-level approach is often used for reviews or social media posts. 

Thankfully, some NLP techniques have now been standardised to help with these issues. One of the most important ones being the use of stemming or lemmatising.

\subsection{Stemming and Lemmatising}
\label{lemmatization}\label{stemming}

Both of these techniques at their core attempt to do the same result: to simplify a word into a more basic form, the difference lies in how that is performed. Stemming trims a word into its word stem, on the other hand, lemmatising is more complex and transforms a word into its its dictionary form. Table \ref{tab:stem and lemma} shows an example of both techniques. The most popular stemming approach is the five-step Porter Stemmer \citep{porter1980algorithm}. This algorithm however does not apply to languages other than English. With regards to lemmatising, rule-based taggers are very commonly found, these probabilistic models have obtained high accuracy and are very commonly used \citep{brill1992simple}. This project makes use of TreeTagger, a multilingual tagger implemented using decision trees \citep{schmid2013probabilistic}. Another tool, SpaCy\footnote{\url{https://spacy.io/}}, an open source multilingual package which includes a lemmatizer was also used. After some comparison, it was found that the SpaCy implementation resulted in a higher rate of word matching so it was ultimately used to compute the results throughout this study.

While stemming is much easier to apply, much more efficient to compute and does not require context, it can miss valuable information, especially when applied to the French language which contains a very high amount of irregular verbs or adjectives. On the other hand, lemmatising is much harder to apply as it requires context of the word in question within the sentence. However it is also much more representative of the meaning of each word. 

\begin{table}[htb]
\caption{Example of Stemming an Lemmatising}
\label{tab:stem and lemma}
\centering
\begin{tabular}{@{}|ccc|@{}}
\toprule
\textbf{Input} & \textbf{Stem} & \textbf{Lemma} \\ \midrule
Livre (to deliver) & livr & livrer \\
Livre (a book) & livr & livre \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Computing Sentiment}

There are two main approaches to performing sentiment analysis, Lexicon based and Machine Learning (ML) based. Both approaches are sometimes combined to form hybrid methods.

\subsubsection{Machine Learning}

Machine learning approaches require a set of labelled training data in order to train a model. Often the quality and size of the training data is key to obtaining results of high accuracy, however these approaches often work on a specific data set and it is much harder to generalise to types of texts that differ significantly from the original data set. For example the weights associated to a machine learning model classifying customer reviews may be very different from classifying news articles as the vocabulary used can be expected to be quite different. When labelled data is unavailable it is possible to train unsupervised machine learning models, however it was found that "supervised methods undoubtedly perform better than other approaches" \citep{navigli2009word}. This study also points out the fact that relying on large training data sets to train those models in different domains, languages and tasks is not realistic to assume as previously pointed out. In fact this study makes the estimation that "to obtain a high-accuracy wide-coverage disambiguation system, we probably need a corpus of about 3.2 million sense-tagged words" which is unrealistic to create manually. In fact the duration of tagging the words of a corpus manually has been estimated previously, "experience [...] suggests that it takes one person one minute to tag one instance of
a word in a corpus (assuming all instances of the same word are tagged in succession)" \citep{do2000designing}. At the rate of one word per minute this task would take about 27 years to obtain a corpus of the size specified.

Popular machine learning models for sentiment analysis include (but is not limited to) Naive Bayes, Support-Vector-Machine (SVM), Neural Nets, K-Nearest neighbours. Some studies have compared some of the above models in specific classification tasks \citep{wawre2016sentiment,ye2009sentiment,seo2020comparative}.

When using a machine learning approach a decision also needs to be made with regards to the output of a model. Most of the sentiment machine learning models are classifiers, giving an output of true or false. This is partially due to most of the available data sets for sentiment classification being labelled as such. It is in fact much harder and more effort to form a labelled corpus such as to train a regression model as the labelling of each article requires an associated score instead of a boolean value. However, while a classifier may work well for customer reviews or tweets, using regression can be favorable in some tasks such as news articles analysis which are more complex in nature and can rarely be summarised to "positive" or "negative". Hybrid methods have been used in this situation by using the output of lexicons into machine learning models and found it to improve the predictions precision \citep{kolchyna2015twitter}.

\subsubsection{Lexicon}

A lexicon based approach relies on a predefined dictionary where each term is associated with a sentiment polarity and possibly a strength (or weight). The neutrality can be one of "positive", "negative" or "neutral", some dictionaries use a uniform weight distribution which essentially counts the occurrences of a lexicons words and others use a weighting system where a weight is computed for each word based on varying reasons. It is also referred to as the "Bag-of-words" approach as it splits documents into tokens as part of the text vectorization process (the process of converting words into a numeric representation). The polarity of a document is then computed by retrieving the number of word occurrences both in the lexicon and document, where each word is associated with a predefined polarity within the lexicon.

A lexicon approach is usually much easier to apply than a machine learning model and is usually more efficient. Studies have compared both methods, some found lexicon approaches to be more precise or found comparable results \citep{dhaoui2017social,mukhtar2018lexicon}, others found machine learning approaches to be more accurate \citep{kolchyna2015twitter,nasim2017sentiment}.

Studies making use of a lexicon based approach generally make use of dictionaries created by computational linguists which are peer reviewed to ensure quality. A number of popular lexica are available in the English language. This is not always the case for other languages such as French, however, it was found that "online tools can be used to get high quality resources with low cost" \citep{abdaoui2017feel}, this study analysed the performance of online translation tools for translating such dictionaries and found an accuracy of over 90\%. Such general dictionaries work well when analysing a general data set, specialised domains such as science or finance will require the use of specialist dictionaries in addition to the general dictionary as words such as "virus" may be associated with a negative sentiment in a general dictionary but is not considered negative is a scientific context.

An issue with this approach is the handling of negation which is still an issue to this day, where words such as "not" or "so" can be used to negate or put emphasis on a word. Lexicon based sentiment analysis fails to handle such cases correctly \citep{le2014distributed}. Hybrid methods using machine learning have been found to keep such relationship between words \citep{mikolov2013exploiting}. On the other hand, algorithms have been developed for better negation handling in lexicon based approaches \citep{diamantini2016negation}.


\section{French Lexica}\label{French Lexicons}

As previously mentioned, there are few published French general dictionaries, three of which have been used in this study.

\subsection{FEEL}\label{chap: feel}

The French Expanded Emotion Lexicon (FEEL) \citep{abdaoui2017feel}, is a lexicon that is built from the semi-automatic translation of the English NRC Word Emotion Association Lexicon \citep{mohammad2013crowdsourcing}. The study started by querying online translation tools to create a first version, then, "a human professional translator manually validated the automatically obtained entries and the associated emotions" \citep{abdaoui2017feel}. It was found that over 94\% of entries were correct in the first version. This study makes available the obtained dictionary and concludes that it is possible to obtain consistent, quality dictionaries with the use of online translation tools.

Word entries of the FEEL lexicon include a polarity of "positive" or negative as well as a set of emotions: joy, surprise, anger, disgust, sadness and fear. The associated weight for each of these emotions is either 0 or 1. More than 80\% of word entries include a single term, with the remaining mostly consisting of two terms and a minority consisting of three terms.

\subsection{Polarimots}\label{chap: polarimots}

Polarimots \citep{gala2012propagation} is a lexicon that has been constructed semi-automatically, consisting of 7,483 entries. An initial set of manually annotated adjectives were created, from which the propagation of polarity was applied. It was found that "spreading polarities is accurate for 78.89\% of the families with a unique adjective" \citep{gala2012propagation}.

With each word entry is associated its word type (noun, verb, adjective or adverb) as well as a reliability metric of either 33\%, 66\% or 100\%. Finally, 100\% of its entries are composed of single words.

\subsection{Diko}\label{chap: diko}

Diko is a lexicon that was built form a game: "LikeIt". LikeIt is "a GWAP (Game With A Purpose) that allows to attribute a positive, negative or neutral value to a term, and thus obtain a resulting polarity" \citep{lafourcade2015collecting}. Because of the nature of the lexicon, Diko is very large. With each word entry, is associated the number of positive, negative and neutral votes. Diko differs significantly from other lexicons as terms are not lemmatized and only 33\% of its terms are single words, with some terms including more than 6 individual words \citep{abdaoui2017feel}. Diko is also the largest and has been found to include 97\% of FEEL terms and 98\% of Polarimots terms.

\section{Statistical Methods}\label{Statistical Methods}

The use of various statistical methods represent a key element for this thesis, as specified in the research objectives (Chapter \ref{chap: Research Objectives}). Such methods provide the link between observed values and identifying potential relationships. The various statistical methods used throughout this thesis will be introduced in the following sections.

\subsection{Core Statistics}

This first section introduces key, core statistic measures which are used throughout this thesis.

\subsubsection{Mean}

The mean is a very well known statistical measure which consists of the sum of the deviation of all of the values in a sample by the size of that sample. In other words, it is the average value in a sample. It is commonly referred to as $\bar{x}$ for a sample as well as $\mu$ for a population. It can be written as such:

\begin{equation}
    \mu = \bar{x} = \frac{1}{N}\sum^{N}_{i=1} x_{i}
\end{equation}
Where N is the sample size and $x_{i}$ represents the ith value in the sample.

\subsubsection{Variance and Standard Deviation}

The standard deviation and variance are related. A common symbol for the standard deviation is $\sigma$ for a population and $S$ for a sample. Given the standard deviation $S$ or $\sigma$ then the variance is its squared value: $S^{2}$ or $\sigma^{2}$.

The standard deviation measures how much data diverges from the mean. A low value indicates that the data is clustered around the mean, on the other hand a high standard deviation indicates that the sample is more spread out. The variance also measures how distributed data is relative to the mean in the same manner, except in much larger values as it is squared.

The mathematical formulas for computing the standard deviation and variance for a sample are the following:
\begin{equation}
    S = \sqrt{\frac{\sum_{i=1}^{N}(x_{i} - \bar{x})^{2}}{N - 1}}
\end{equation}
\begin{equation}
    S^2 = \frac{\sum_{i=1}^{N}(x_{i} - \bar{x})^{2}}{N - 1}
\end{equation}

\subsubsection{Skewness}

Similar to the variance and standard deviation, the skewness also measures data distribution. The skewness measures the degree of asymmetry from a normal distribution, in other words, it quantifies the difference between a sample distribution and the normal distribution.

Hence, the skewness for a normal distribution is 0. A positive skewness indicates that the data is skewed to the right, in other words the right tail is longer than the left. On the other hand, a negative skewness indicates that the data is skewed to the left. Figure \ref{skewness fig} illustrates this concept. Mathematically, it is written as such:

\begin{equation}
    skewness = \frac{\sum_{i=1}^{N}(x_{i} - \bar{x})^{3}}{(N - 1) * S^{3}}
\end{equation}
Where N is the sample size, $x_{i}$ is the ith data point from the sample, $\bar{x}$ is the mean and $S$ is the standard deviation.

\begin{figure}[h!]
      \centering
      \includegraphics[scale=0.8]{lit_review/skewness.png}
      \caption{Skewness Visualisation}
      \label{skewness fig}
      \emph{Source: Wikipedia}
\end{figure}

\subsubsection{Kurtosis}

Kurtosis is a measure of whether a sample is heavily tailed or lightly tailed relative to a normal distribution. A high kurtosis value infers that a data set will have a large amount of outliers. On the other hand a low kurtosis value indicates a data set with very few outliers. The kurtosis for a standard normal distribution is 3, hence a metric often used is referred to as the "excess kurtosis" and takes the value $kurtosis - 1$.

Mathematically, the kurtosis value is calculated as such:
\begin{equation}
    kurtosis = \frac{1}{N - 1} \frac{\sum_{i=1}^{N}(x_{i} - \bar{x})^{4}}{S^{4}}
\end{equation}

\subsubsection{P-Value}

The P-Value is used in null hypothesis significance testing. Assuming a null hypothesis $H_{0}$, a hypothesis of no difference, and a second hypothesis $H_{1}$, representing the opposite of the null hypothesis. The P-Value is the probability of finding greater results when the null hypothesis $H_{0}$ is true. A level of significance is picked, typically one of 0.1, 0.05 or 0.01. A p-value below the threshold, then we can conclude that there is significant evidence to reject the null hypothesis $H_{0}$. This study uses a significance level of 5\% (p-value of 0.05) by default unless specified otherwise.

\subsubsection{Correlation}

This study makes use of the Pearson product-moment correlation coefficient. Correlation (r) quantifies linear relationship between two data sets. Its values ranges between -1 and 1. A correlation value of zero indicates no linear relationship between the two sets of data. A positive correlation indicates that both variables move in the same direction, it shows that when one becomes larger or smaller, so does the other. On the other hand, a negative correlation shows the opposite: both variables move in the opposite directions. 
A larger distance between zero and the correlation value indicates a stronger relationship, this is true for both positive and negative correlations. The coefficient can be computed using the formula below.

\begin{equation}
    r = \frac{\sum_{i=1}^{N}(x_{i} - \bar{x})(y_{i} - \bar{y})}{\sqrt{\sum_{i=1}^{N}(x_{i} - \bar{x})^{2}\sum_{i=1}^{N}(y_{i} - \bar{y})^{2}}}
\end{equation}
Note that both sets of data must be of the same size. Here N is the sample size of both x and y data sets, $x_{i}$ and $y_{i}$ are the ith data point for data sets x and y respectively, finally $\bar{x}$ and $\bar{y}$ are the means for both data sets.

\subsubsection{Z-Score}

The Z-Score (Z), also known as the standard score, is a value which shows a measure of the a value's relationship to the mean of the data set it belongs to. It represents the number of standard deviations by which a value is above or below the mean value.

\begin{equation}
    Z = \frac{x - \bar{x}}{S}
\end{equation}
Where x is a value of the data set, $\bar{x}$ is the mean and $S$ is the standard deviation.

\subsection{TODO: VAR / Causality Test}
TODO
%- main statistical measures
%- correlation 
%- statistical significance (look into durbin watson), hypothesis testing
%- F-statistic with p-value
%- unit root test
%- R\^2
%- granger causuality test
%- VAR ?
%- variance, z-score
